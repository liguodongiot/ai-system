



- [为实时应用构建人工智能推理引擎的挑战](https://redis.io/blog/the-challenges-in-building-an-ai-inference-engine-for-real-time-applications/)
- [Full Stack Optimization of Transformer Inference: a Survey](https://arxiv.org/pdf/2302.14017)

- [TensorRT](https://mmdeploy.readthedocs.io/zh-cn/v1.3.1/tutorial/06_introduction_to_tensorrt.html)

- [LLM Inference Serving: Survey of Recent Advances and Opportunities](https://arxiv.org/pdf/2407.12391v1)

- [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/pdf/2403.02310)

- [sarathi-serve](https://github.com/microsoft/sarathi-serve)：低延迟 高吞吐的llm服务引擎

- [Llumnix: Dynamic Scheduling for Large Language Model Serving](https://github.com/AlibabaPAI/llumnix?tab=readme-ov-file)：大语言模型服务的动态调度



- [大模型推理框架概述](https://juejin.cn/post/7286676030965317668)

- [模型推理](https://openmlsys.github.io/chapter_model_deployment/model_inference.html)：机器学习系统：设计和实现

- [推理系统](https://chenzomi12.github.io/04Inference01Inference/README.html)：AI 系统