





## 激活值

在深度学习中，激活值（Activation） 是指神经网络中每个神经元在接收到输入后产生的输出值。激活值通常表示神经元的兴奋程度，它决定了神经元是否会被激活并传递信号给下一层的神经元。

激活值的计算通常包括两个主要部分：

加权和（Weighted Sum）：神经元会将其输入与相应的权重相乘，然后将所有加权输入求和。这个加权和是神经元接收到的总输入。

激活函数（Activation Function）：接着，神经元将加权和输入到激活函数中，以产生最终的激活值。激活函数通常用来引入非线性性质，允许神经网络模型学习复杂的关系。

常见的激活函数包括 Sigmoid、ReLU（Rectified Linear Unit）、Tanh（双曲正切） 等，它们在不同情况下可以产生不同的激活值范围和非线性特性，有助于神经网络适应不同类型的数据和任务。

激活值在神经网络的前向传播过程中不断传递，最终用于生成网络的输出。神经网络通过调整权重和激活函数来学习如何从输入数据中提取有用的特征和信息，以完成各种机器学习任务，如分类、回归、图像处理等。


## 梯度


在深度学习中，梯度（Gradient） 是指一个多变量函数在某一点的导数或偏导数的向量。它表示了函数在该点处的变化率和方向。梯度常用于优化算法中，特别是在神经网络的训练过程中，以调整模型参数来最小化损失函数。

具体来说，如果有一个损失函数（通常表示为 L），该函数依赖于模型的参数（例如权重和偏置），那么损失函数对每个参数的梯度告诉我们：如果我们微调该参数的值，损失函数会如何变化。梯度是一个向量，它包含了每个参数的偏导数。

数学上，如果有一个函数 f(x1, x2, ..., xn)，其中 x1, x2, ..., xn 是模型的参数，那么该函数在某一点 (x1, x2, ..., xn) 的梯度可以表示为 (∂f/∂x1, ∂f/∂x2, ..., ∂f/∂xn)。这个向量指示了函数在每个参数方向上的变化率，即函数变化最快的方向。

在深度学习中，梯度下降是一种常用的优化方法，它使用梯度信息来更新模型参数，以逐渐减小损失函数的值，从而使模型更好地拟合训练数据。反向传播算法用于高效地计算神经网络中每个参数的梯度，使得神经网络能够进行有效的训练和学习。

总之，梯度在深度学习中是一个非常重要的概念，它用于指导模型参数的更新，以最小化损失函数，从而提高模型的性能。




### 梯度流（gradient flow）




###  卷积核 


### 滤波器 


###  通道


### Layer














