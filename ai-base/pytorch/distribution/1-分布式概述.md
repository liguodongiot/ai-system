











## 基于RPC的分布式训练



许多训练范式不适合数据并行，例如：参数服务器范式、分布式流水线并行、具有多个观察者或代理的强化学习应用程序等。

torch.distributed.rpc旨在支持一般的分布式训练场景。



torch.distributed.rpc 有四个主要支柱：

- RPC ： 支持在远程worker上运行给定的函数。

- RRef ：有助于管理远程对象的生命周期。 引用计数协议在 RRef 注释中介绍。

- 分布式 Autograd ：将 autograd 引擎扩展到机器边界之外。 更多详细信息请参阅分布式 Autograd 设计。

- 分布式优化器：自动联系所有参与的workers，使用分布式 autograd 引擎计算的梯度来更新参数。


RPC教程如下：

- 分布式 RPC 框架入门教程：首先使用一个简单的强化学习 (RL) 示例来演示 RPC 和 RRef。 然后，它将基本的分布式模型并行应用到 RNN 示例中，以展示如何使用分布式 autograd 和分布式优化器。

- 使用分布式 RPC 框架实现参数服务器教程：借鉴了 HogWild 的精神！ 训练并将其应用于异步参数服务器（PS）训练应用程序。

- 使用 RPC 的分布式流水线并行教程：将单机流水线并行示例（在单机模型并行最佳实践中介绍）扩展到分布式环境，并展示了如何使用 RPC 实现它。

- 使用异步执行实现批量 RPC 处理教程：演示了如何使用 @rpc.functions.async_execution 装饰器实现 RPC 批量处理，这有助于加快推理和训练速度。 它使用类似于上述教程 1 和 2 中的 RL 和 PS 示例。

- 将分布式数据并行与分布式 RPC 框架相结合教程：演示了如何将 DDP 与 RPC 相结合，以使用分布式数据并行与分布式模型并行性相结合来训练模型。







